{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c9266b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cdc66cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\JOKER\\Desktop\\face recognition'  # here collected dataset that we used for recognition\n",
    "images = []                                              # Only for images\n",
    "classNames = []  # only for images names  note: image withrespective name of images\n",
    "myList = os.listdir(path)  #taking the path access\n",
    "for cl in myList:\n",
    "    curImg = cv2.imread(f'{path}/{cl}')     #accessing the all images \n",
    "    images.append(curImg)                     # storing the all images\n",
    "    classNames.append(os.path.splitext(cl)[0])    # storing the names of the images \n",
    "\n",
    "#encodeListKnown=np.load('encodeface.npy')    # loading the encoding file of the collected images it will store in array formate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138f4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findEncodings(images):       #this block is used for the training the images in the digital format \n",
    "    encodeList = []              # empty list for storing the encoded images for recognition\n",
    "    for img in images:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)            # converting images into original format \n",
    "        encode = face_recognition.face_encodings(img)[0]        # storing the encoded images in encode\n",
    "        encodeList.append(encode)                               # storing the all encoded images\n",
    "    return encodeList\n",
    "encodeListKnown = findEncodings(images)                      # return the encoded data \n",
    "#np.save('encodeface.npy',encodeListKnown)   # here encoded data takes time so for reduce the time we save the encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2a87332",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)    # web cam or camara accessing command\n",
    "while True:\n",
    "    ret,frame= cap.read()                            # here frame is the photo for every secound\n",
    "    imgS = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    facesCurFrame = face_recognition.face_locations(imgS)        # loacting the face in the frame\n",
    "    encodesCurFrame = face_recognition.face_encodings(imgS, facesCurFrame)  # encoded the faces in the frame\n",
    "    for encodeFace, faceLoc in zip(encodesCurFrame, facesCurFrame):\n",
    "            matches = face_recognition.compare_faces(encodeListKnown, encodeFace)   # compare the encoded face from cam and encoded faces from colleceted data\n",
    "            faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)  # find the which position of the faces is matched such if face is matched at six face then return the 6 values in array format\n",
    "            matchIndex = np.argmin(faceDis)  # convert the array to normal number such as integer format\n",
    "    \n",
    "            if matches[matchIndex]:     \n",
    "                name = classNames[matchIndex].upper()   # finding the name via using the matchIndex number\n",
    "                x,y,w,h = faceLoc        # loacation of the faces which are matched in x/4 ratio format\n",
    "                cv2.rectangle(frame, (y,x),(h,w), (0, 255, 0), 2)    # creating the rectangle on matched faces\n",
    "                cv2.rectangle(frame, (y,w-35),(h,w), (0, 255, 0), cv2.FILLED)  # this is used for putting the recognized name\n",
    "                # box for face and box for text\n",
    "                cv2.putText(frame, name, (h + 6, w - 6), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)  # name text\n",
    "\n",
    "    cv2.imshow('testing faces',frame)   # return the frame after recognition\n",
    "    if cv2.waitKey(1) & 0xff==ord('q'):   # used for closing the loop \n",
    "        break\n",
    "cap.release()        # deacitvating the cam which we used now\n",
    "cv2.destroyAllWindows()   # closing the all related windows which are opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c735e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f62a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_recognition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
